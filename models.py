import os
import pandas as pd
import seaborn as sns
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from keras.callbacks import ReduceLROnPlateau
from keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from lib.preprocessing import *
from lib.ham_ho_tro import *




class Model:
    '''class nÃ y Ä‘á»ƒ cho cÃ¡c class con káº¿ thá»«a'''
   
    def __init__(self):
        pass
    
    def build_model(self):
        pass
    
    def preprocessing(self):
        pass
    def train(self):
        pass

    def resume_training(self, epochs_to_train):
        if self.continue_studying == False:
            raise Exception('Báº¡n Ä‘Ã£ cÃ i Ä‘áº·t khÃ´ng há»c tiáº¿p ngay tá»« Ä‘áº§u, vÃ  self.X, self.Y Ä‘Ã£ Ä‘Æ°á»£c xÃ³a Ä‘i Ä‘á»ƒ tiáº¿c kiá»‡m bá»™ nhá»›')
        # Äáº£m báº£o cÃ³ sá»± huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³
        if self.history is None:
            print("KhÃ´ng cÃ³ quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ tiáº¿p tá»¥c.")
            return
        if epochs_to_train < self.history.epoch[-1]:
            print("Sá»‘ epoch cáº§n tiáº¿p tá»¥c huáº¥n luyá»‡n pháº£i lá»›n hÆ¡n sá»‘ epoch Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³.")
            print("Sá»‘ epoch Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³:", self.history.epoch[-1])
            print("Sá»‘ epoch cáº§n tiáº¿p tá»¥c huáº¥n luyá»‡n:", epochs_to_train)
            print("hay lÃ  báº¡n cháº¡y láº¡i tá»« Ä‘áº§u ğŸ˜ƒ")
            return None
        try:
            # Tiáº¿p tá»¥c huáº¥n luyá»‡n tá»« epoch cuá»‘i cÃ¹ng cá»§a láº§n trÆ°á»›c
            self.history = self.model.fit(self.X, self.Y, validation_split=0.2, epochs = epochs_to_train, batch_size=64, initial_epoch=self.history.epoch[-1]+1)
            print("Tiáº¿p tá»¥c huáº¥n luyá»‡n thÃ nh cÃ´ng.")
        except Exception as e:
            print("ÄÃ£ xáº£y ra lá»—i khi tiáº¿p tá»¥c huáº¥n luyá»‡n:", str(e))
    
    def results(self):
        plot_acc_and_loss(self.history, self.epochs)
        ev = self.evaluate(data=(self.X_test, self.Y_test))
        print('\n', ev[0], '\n')
        print(ev[1], '\n')
        print(ev[2], '\n')
        plt.figure(figsize=(12,8))
        ax = plt.axes()
        sns.heatmap(ev[1], ax = ax, cmap = 'BuGn', fmt="d", annot=True)
        ax.set_ylabel('True emotion')
        ax.set_xlabel('Predicted emotion')
        plt.show()
        if self.continue_studying == False:
            del self.X_test, self.Y_test  # giáº£i phÃ³ng bá»™ nhá»›
    
    def predict(self):
        pass

    def evaluate(self, path=None, data=None):
        '''thÆ° má»¥c cáº§n kiá»ƒm tra Ä‘áº·t trong thÆ° má»¥c data, chá»‰ cáº§n ghi mÃ¬nh tÃªn thÆ° má»¥c, khÃ´ng cáº§n ghi Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§'''
        if type(path) == str:
            path = [path]
        if path != None:
            sound_list = audio_preprocessing(path, self.select_audio_preprocessing, self.n_sample)
            X, Y = audio_feature(sound_list, self.time_or_frequency, self.audio_features, self.n_chroma, self.n_mfcc, self.hs, self.TB)
            y_pred = self.predict(self, data=X)
        elif data != None:
            X, Y = data
            y_pred = decode(self.model.predict(X))
        else:
            raise Exception('Báº¡n chÆ°a chá»n Ä‘Æ°á»ng dáº«n hoáº·c dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n')
        eva = self.model.evaluate(X, Y, verbose=2)
        y = decode(Y)
        mlb = list(set(y))
        mlb.sort()
        lb=[]
        for i in mlb:
            lb.append(decode_emotion(i))
        cm=confusion_matrix(y, y_pred)
        cm_df = pd.DataFrame(cm, lb, lb)                      
        return eva, cm_df, classification_report(y, y_pred, target_names=lb)

    def run(self, epochs = 50):
        self.preprocessing()
        self.train(epochs)
        self.results()



class cnn(Model):
    '''
    path_data: lÃ  Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o, cÃ³ thá»ƒ lÃ  Ä‘áº¿n má»™t 
    hay nhiá»u thÆ° má»¥c náº¿u nhiá»u thÆ° má»¥c thÃ¬ Ä‘áº·t trong má»™t list
    
    num_dimensions: quyáº¿t Ä‘á»‹nh xem cÃ¡ch há»c mÃ´ hÃ¬nh cnn lÃ  há»c dáº¡ng áº£nh (2d) hay láº¥y trung bÃ¬nh
    cÃ¡c tÃ­nh cháº¥t (1d), nÃ³ nháº­n 2 giÃ¡ trá»‹ lÃ  num_dimensions = '1d' hoáº·c num_dimensions = '2d'
    
    audio_features: lÃ  cÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh báº¡n muá»‘n Ä‘Æ°a vÃ o Ä‘á»ƒ há»c. vd: audio_features = ('mfcc', 'zcr', 'mel')
    
    '''
    continue_studying = False
    n_chroma = 12
    n_mfcc = 20
    n_sample = 5*22050
    hs = [1, 1, 1, 1, 1]
    
    select_audio_preprocessing = {
        'sequentially': True, # default
        'normalize': True,
        'trim': True,
        'reduce_noise': False,
        'stretch': False,
        'picth' : False,
        'add_noise': False,
        'shift': False,
        'remove_slient' : False, # Ä‘á»c láº¡i cáº£nh bÃ¡o cá»§a An
        'split_pad_data': True
    }
    
    
    def __init__(self, path_data, num_dimensions = None, audio_features = None):
        self.path_data = path_data
        self.num_dimensions = num_dimensions
        self.audio_features = audio_features
        if type(audio_features) == str:
            self.audio_features =[audio_features]
        if type(path_data) == str:  
            if os.path.isfile(path_data):
                raise ValueError('ÄÆ°á»ng dáº«n pháº£i lÃ  má»™t thÆ° má»¥c')
            self.path_data = [path_data]
        input_shape = cal_input_shape(num_dimensions, audio_features, self.n_sample, self.n_chroma, self.n_mfcc)
        print('\nkÃ­ch thÆ°á»›c ma tráº­n Ä‘áº§u vÃ o Ä‘Ã£ tÃ­nh toÃ¡n:', input_shape, '\n\n')
        self.loss = 'categorical_crossentropy'
        self.build_model(input_shape)
    
    def build_model(self, input_shape):
        if self.num_dimensions == '1d':
            model=Sequential()
            model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=input_shape))
            model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

            model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
            model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

            model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
            model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
            model.add(Dropout(0.2))

            model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
            model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

            model.add(Flatten())
            model.add(Dense(units=32, activation='relu', kernel_regularizer = regularizers.l1(0.01)))
            model.add(Dropout(0.3))

            model.add(Dense(units=7, activation='softmax', kernel_regularizer=regularizers.l1(0.01)))
            model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

            self.model = model
            
        elif self.num_dimensions == '2d':
            # ...
            pass
        else:
            raise ValueError('num_dimensions chá»‰ nháº­n 2 giÃ¡ trá»‹ lÃ  1d hoáº·c 2d')
    
    def preprocessing(self):
        sound_list = audio_preprocessing(self.path_data, self.select_audio_preprocessing, self.n_sample)
        X, Y = audio_feature(sound_list, self.num_dimensions, self.audio_features, self.n_chroma, self.n_mfcc, self.hs)
        print('\nkÃ­ch thÆ°á»›c ma tráº­n qua bá»™ tiá»n sá»­ lÃ­:', X.shape, Y.shape)
        print('Náº¿u khÃ¡c kÃ­ch thÆ°á»›c ma tráº­n Ä‘Ã£ tÃ­nh toÃ¡n thÃ¬ cÃ³ váº¥n Ä‘á»\n')
        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size = 0.2)

    def train(self, epochs = 100):
        self.epochs = epochs
        rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)
        self.history = self.model.fit(self.X_train, self.Y_train, batch_size=64, epochs=epochs, validation_data=(self.X_test, self.Y_test), callbacks=[rlrp])
        # self.model.save('storage/LSTM/LSTM.keras') # khÃ´ng dÃ¹ng cÃ¡ch nÃ y Ä‘á»ƒ lÆ°u mÃ´ hÃ¬nh ná»¯a
        if self.continue_studying == False:
            del self.X_train, self.Y_train  # giáº£i phÃ³ng bá»™ nhá»›

    def predict(self, path=None, data=None):
        '''file nhá»› ghi Ä‘áº§y Ä‘á»§ Ä‘Æ°á»ng dáº«n, cÃ³ cáº£ chá»¯ data náº¿u Ä‘áº·t trong thÆ° má»¥c data'''
        if path is None and data is None:
            raise Exception('Báº¡n chÆ°a chá»n Ä‘Æ°á»ng dáº«n hoáº·c dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n')
        if type(path) == str:
            path = [path]
        sound_list = audio_preprocessing(path, self.select_audio_preprocessing, self.n_sample, predict = True, datax = data)
        X, _ = audio_feature(sound_list, self.num_dimensions, self.audio_features, self.n_chroma, self.n_mfcc, self.hs)
        return decode(self.model.predict(X))



class lstm(Model):
    '''
    path_data: lÃ  Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o, cÃ³ thá»ƒ lÃ  Ä‘áº¿n má»™t 
    hay nhiá»u thÆ° má»¥c náº¿u nhiá»u thÆ° má»¥c thÃ¬ Ä‘áº·t trong má»™t list.
    
    time_or_frequency: báº¡n cÃ³ thá»ƒ chá»n há»c theo trá»¥c thá»i gian hay táº§n sá»‘, nÃ³ nháº­n 2 giÃ¡ trá»‹
    lÃ  time_or_frequency = 'time' hoáº·c time_or_frequency = 'frequency'
    
    audio_features: lÃ  cÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh báº¡n muá»‘n Ä‘Æ°a vÃ o Ä‘á»ƒ há»c. vd: audio_features = ('mfcc', 'zcr', 'mel')
    
    TB: khi time_or_frequency = 'frequency' TB mang 2 giÃ¡ trá»‹ TB = True hoáº·c TB = False
    khi time_or_frequency = 'time' thÃ¬ TB cÃ³ thá»ƒ mang giÃ¡ trá»‹ True hoáº·c False hoáº·c tuple hay list
    cÃ¡c giÃ¡ trá»‹ bool Ä‘áº¡i diá»‡n cho tÃ­nh trung bÃ¬nh cá»§a (chroma_stft, mfcc, mel). vd: TB = (True, False, True)
    '''
    continue_studying = False
    n_chroma = 12
    n_mfcc = 20
    n_sample = 5*22050
    hs = [1, 1, 1, 1, 1]   # há»‡ sá»‘ nhÃ¢n vá»›i cÃ¡c tÃ­nh cháº¥t: zcr, chroma_stft, mfcc, rms, mel
    
    select_audio_preprocessing = {
        'sequentially': True, # default
        'normalize': True,
        'trim': True,
        'reduce_noise': False,
        'stretch': False,
        'picth' : False,
        'add_noise': False,
        'shift': False,
        'remove_slient' : False, # Ä‘á»c láº¡i cáº£nh bÃ¡o cá»§a an
        'split_pad_data': True
    }
    
    
    def __init__(self, path_data, time_or_frequency = None, audio_features = None, TB = False):
        self.path_data = path_data
        self.time_or_frequency = time_or_frequency
        self.audio_features = audio_features
        self.TB = TB
        if type(audio_features) == str:
            self.audio_features =[audio_features]
        if type(path_data) == str:  
            if os.path.isfile(path_data):
                raise ValueError('ÄÆ°á»ng dáº«n pháº£i lÃ  má»™t thÆ° má»¥c')
            self.path_data = [path_data]
        input_shape = cal_input_shape(time_or_frequency, audio_features, self.n_sample, self.n_chroma, self.n_mfcc, TB)
        print('\nkÃ­ch thÆ°á»›c ma tráº­n Ä‘áº§u vÃ o Ä‘Ã£ tÃ­nh toÃ¡n:', input_shape, '\n\n')
        self.loss = 'categorical_crossentropy'
        self.build_model(input_shape)
    
    def build_model(self, input_shape):
        model = Sequential([
            LSTM(256, return_sequences=False, input_shape=input_shape),
            Dropout(0.2),
            Dense(128, activation='relu'),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(7, activation='softmax')
        ])
        model.compile(loss=self.loss, optimizer='adam', metrics=['accuracy'])
        self.model = model
    
    def preprocessing(self):
        sound_list = audio_preprocessing(self.path_data, self.select_audio_preprocessing, self.n_sample)
        X, Y = audio_feature(sound_list, self.time_or_frequency, self.audio_features, self.n_chroma, self.n_mfcc, self.hs, self.TB)
        print('\nkÃ­ch thÆ°á»›c ma tráº­n qua bá»™ tiá»n sá»­ lÃ­:', X.shape, Y.shape)
        print('Náº¿u khÃ¡c kÃ­ch thÆ°á»›c ma tráº­n Ä‘Ã£ tÃ­nh toÃ¡n thÃ¬ cÃ³ váº¥n Ä‘á»\n')
        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size = 0.2)

    def train(self, epochs = 100):
        self.epochs = epochs
        self.history = self.model.fit(self.X_train, self.Y_train, validation_data=(self.X_test, self.Y_test), epochs=epochs, batch_size=64)
        # self.model.save('storage/LSTM/LSTM.keras') # khÃ´ng dÃ¹ng cÃ¡ch nÃ y Ä‘á»ƒ lÆ°u mÃ´ hÃ¬nh ná»¯a
        if self.continue_studying == False:
            del self.X_train, self.Y_train  # giáº£i phÃ³ng bá»™ nhá»›
    
    def predict(self, path=None, data=None):
        '''file nhá»› ghi Ä‘áº§y Ä‘á»§ Ä‘Æ°á»ng dáº«n, cÃ³ cáº£ chá»¯ data náº¿u Ä‘áº·t trong thÆ° má»¥c data'''
        if path is None and data is None:
            raise Exception('Báº¡n chÆ°a chá»n Ä‘Æ°á»ng dáº«n hoáº·c dá»¯ liá»‡u Ä‘á»ƒ dá»± Ä‘oÃ¡n')
        if type(path) == str:
            path = [path]
        sound_list = audio_preprocessing(path, self.select_audio_preprocessing, self.n_sample, predict = True, datax = data)
        X, _ = audio_feature(sound_list, self.time_or_frequency, self.audio_features, self.n_chroma, self.n_mfcc, self.hs, self.TB)
        return decode(self.model.predict(X))




